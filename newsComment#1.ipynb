{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버뉴스 본문, 댓글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 뉴스 제목, 본문, 날짜, url 수집**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 출력할 페이지수 입력하시오: 2\n",
      "검색어 입력: 이춘재\n",
      "시작날짜 입력(2019.09.19):2019.09.19\n",
      "끝날짜 입력(2019.10.19):2019.09.30\n",
      "1\n",
      "https://search.naver.com/search.naver?where=news&query=이춘재&sort=0&ds=2019.09.19&de=2019.09.30&nso=so%3Ar%2Cp%3Afrom20190919to20190930%2Ca%3A&start=1\n",
      "'NoneType' object has no attribute 'get_text'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8: expected 5 fields, saw 10\\nSkipping line 9: expected 5 fields, saw 7\\nSkipping line 10: expected 5 fields, saw 7\\nSkipping line 17: expected 5 fields, saw 11\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          years     company  \\\n",
      "0   2019.09.30.        노컷뉴스   \n",
      "1   2019.09.30.         YTN   \n",
      "2   2019.09.30.       마이데일리   \n",
      "3   2019.09.30.       헤럴드경제   \n",
      "4   2019.09.30.        JTBC   \n",
      "5   2019.09.30.        JTBC   \n",
      "6   2019.09.29.         YTN   \n",
      "7   2019.09.29.         뉴스1   \n",
      "8   2019.09.29.        세계닷컴   \n",
      "9   2019.09.29.      edaily   \n",
      "10  2019.09.29.  MoneyToday   \n",
      "11  2019.09.29.         채널A   \n",
      "12  2019.09.28.        JTBC   \n",
      "13  2019.09.28.        부산일보   \n",
      "\n",
      "                                                title  \\\n",
      "0                        이춘재 8차 대면조사…4차 사건 DNA 결과는 아직   \n",
      "1                             이춘재 대면조사 재개...여전히 혐의 부인   \n",
      "2               '화성연쇄살인사건' 이춘재 다룬 '그알'…2049 주말 시청률 1위   \n",
      "3   ‘그알’ 화성연쇄살인 2부, 처제성폭행 사건 분석 통한 이춘재의 기이한 발언과 행적 추적   \n",
      "4                  '화성사건' 발생 2년 전…\"이춘재 닮은 20대가 흉기 위협\"   \n",
      "5                     마주친 시기는 이춘재 군 복무 시절…휴가 중 범행 가능성   \n",
      "6                  화성 연쇄살인 목격자 버스안내원, 이춘재 사진에 \"범인 맞다\"   \n",
      "7                      화성살인사건 핵심 목격자 “범인 얼굴, 이춘재와 같다”   \n",
      "8                        화성사건 목격한 ‘버스안내양’ “이춘재 범인 맞다”   \n",
      "9                   이춘재 사진 본 화성연쇄살인사건 '버스안내양' \"범인 맞다\"   \n",
      "10                          이춘재 사진 본 화성 버스안내양 \"범인 맞다\"   \n",
      "11                     이춘재, 안양교도소 이감 무산…진술 놓고 ‘수사 난항’   \n",
      "12                  '화성 사건' 목격자 최면 조사…이춘재 사진에 \"범인 맞아\"   \n",
      "13            화성연쇄살인범 목격한 '버스 안내양', 이춘재 사진 보고 \"범인 맞다\"   \n",
      "\n",
      "                                             contents  \\\n",
      "0   화성 7차 사건 목격자 '버스 안내양' 이춘재 지목[CBS노컷뉴스 고무성 기자](사...   \n",
      "1   화성 연쇄 살인사건을 수사하고 있는 경찰이 유력한 용의자인 이춘재에 대한 대면조사를...   \n",
      "2   [마이데일리 = 이예은 기자] 화성연쇄살인사건을 다룬 SBS 시사 프로그램 '그것이...   \n",
      "3   [헤럴드경제 =서병기 선임기자]화성 연쇄살인사건을 다룬 SBS ‘그것이 알고싶다’가...   \n",
      "4   동영상 뉴스       [앵커]첫 번째 화성연쇄살인사건은 1986년 9월에 일어났습...   \n",
      "5   동영상 뉴스       \"머리 짧고 몽타주 닮아\"…이춘재 제대 전도 살핀다[앵커]목...   \n",
      "6   화성 연쇄살인 7차 사건의 목격자인 버스 안내원 엄 모 씨가 최면 조사에서 유력한 ...   \n",
      "7   7차사건때 용의자 목격한 버스안내양 최면진술4차사건 당시 또다른 목격자 존재여부도 ...   \n",
      "8   7차 사건 당시 화성연쇄살인사건 용의자 몽타주 수배전단의 모습. 뉴시스    경기도...   \n",
      "9   [이데일리 박지혜 기자] ‘화성 연쇄살인 사건’의 목격자인 ‘버스 안내양’이 최면 ...   \n",
      "10  [머니투데이 김영상 기자] [7차 사건 목격자 버스 안내양, \"기억 속의 용의자 맞...   \n",
      "11  동영상 뉴스        화성 연쇄살인 사건 수사 소식입니다.   유력 용의자로 이...   \n",
      "12  동영상 뉴스       [앵커]화성연쇄살인 사건과 관련해 최근 최면 조사에 나선 목...   \n",
      "13  화성연쇄살인 7차 사건 당시 용의자 몽타주 수배전단. 연합뉴스화성연쇄살인사건 당시 ...   \n",
      "\n",
      "                                                 link  \n",
      "0   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "1   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "2   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "3   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "4   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "5   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "6   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "7   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "8   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "9   https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "10  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "11  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "12  https://news.naver.com/main/read.nhn?mode=LSD&...  \n",
      "13  https://news.naver.com/main/read.nhn?mode=LSD&...  \n"
     ]
    }
   ],
   "source": [
    "RESULT_PATH = 'C:/Users/snari/python study/5주차/'\n",
    "now = datetime.now() #파일이름 현 시간으로 저장하기\n",
    "\n",
    "def get_news(n_url):\n",
    "    news_detail = []\n",
    "\n",
    "    breq = requests.get(n_url)\n",
    "    bsoup = BeautifulSoup(breq.content, 'html.parser')\n",
    "\n",
    "    title = bsoup.select('h3#articleTitle')[0].text  \n",
    "    news_detail.append(title) #제목\n",
    "\n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:11]\n",
    "    news_detail.append(pdate) #날짜\n",
    "\n",
    "    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \")\n",
    "    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\")\n",
    "    news_detail.append(btext.strip()) #댓글 본문\n",
    "  \n",
    "    news_detail.append(n_url) #url\n",
    "    \n",
    "    pcompany = bsoup.select('#footer address')[0].a.get_text()\n",
    "    news_detail.append(pcompany) #신문사\n",
    "\n",
    "    return news_detail\n",
    "\n",
    "def crawler(maxpage,query,s_date,e_date):\n",
    "\n",
    "    s_from = s_date.replace(\".\",\"\")\n",
    "    e_to = e_date.replace(\".\",\"\")\n",
    "    page = 1\n",
    "    maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    f = open(\"C:/Users/snari/python study/5주차/contents_text.txt\", 'w', encoding='utf-8')\n",
    "    \n",
    "    while page < maxpage_t:\n",
    "    \n",
    "        print(page)\n",
    "    \n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=0&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page)\n",
    "        \n",
    "        req = requests.get(url)\n",
    "        print(url)\n",
    "        cont = req.content\n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "            #print(soup)\n",
    "    \n",
    "        for urls in soup.select(\"._sp_each_url\"):\n",
    "            try :\n",
    "                #print(urls[\"href\"])\n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    #print(urls[\"href\"])\n",
    "                    news_detail = get_news(urls[\"href\"])\n",
    "                        # pdate, pcompany, title, btext\n",
    "                    f.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(news_detail[1], news_detail[4], news_detail[0], news_detail[2],news_detail[3]))  # new style\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        page += 10\n",
    "    \n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "def excel_make():\n",
    "    data = pd.read_csv(RESULT_PATH+'contents_text.txt', sep='\\t',header=None, error_bad_lines=False)\n",
    "    data.columns = ['years','company','title','contents','link']\n",
    "    print(data)\n",
    "    \n",
    "    xlsx_outputFileName = '%s-%s-%s  %s시 %s분 %s초 result.xlsx' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "    #xlsx_name = 'result' + '.xlsx'\n",
    "    data.to_excel(RESULT_PATH+xlsx_outputFileName, encoding='utf-8')\n",
    "\n",
    "\n",
    "def main():\n",
    "    maxpage = input(\"최대 출력할 페이지수 입력하시오: \") \n",
    "    query = input(\"검색어 입력: \")\n",
    "    s_date = input(\"시작날짜 입력(2019.09.19):\")  #2019.09.19\n",
    "    e_date = input(\"끝날짜 입력(2019.10.19):\")   #2019.10.19\n",
    "    crawler(maxpage,query,s_date,e_date) #검색된 네이버뉴스의 기사내용을 크롤링합니다. \n",
    "    \n",
    "    excel_make() #엑셀로 만들기 \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 댓글 수집**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 여러 리스트들을 하나로 묶어 주는 함수입니다.\\ndef flatten(l):\\n    flatList = []\\n    for elem in l:\\n        # if an element of a list is a list\\n        # iterate over this list and add elements to flatList \\n        if type(elem) == list:\\n            for e in elem:\\n                flatList.append(e)\\n        else:\\n            flatList.append(elem)\\n    return flatList\\n\\n\\n# 리스트 결과입니다.\\nflatten(List)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULT_PATH ='C:/Users/snari/python study/5주차/'  #결과 저장할 경로\n",
    "now = datetime.now()\n",
    "\n",
    "# 댓글을 달 빈 리스트를 생성합니다.\n",
    "List=[]\n",
    "result={}\n",
    "# 라이브러리를 로드합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "# 네이버 뉴스 url을 입력합니다.\n",
    "url=\"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=102&oid=016&aid=0001591741\" #referer 다 들어가서 분석\n",
    "\n",
    "oid=url.split(\"oid=\")[1].split(\"&\")[0]\n",
    "aid=url.split(\"aid=\")[1]\n",
    "page=1    \n",
    "header = {\n",
    "    \"User-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "    \"referer\":url,\n",
    "    \n",
    "} \n",
    "while True :\n",
    "    c_url=\"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback=jQuery1707138182064460843_1523512042464&lang=ko&country=&objectId=news\"+oid+\"%2C\"+aid+\"&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=\"+str(page)+\"&refresh=false&sort=FAVORITE\" \n",
    "# 파싱하는 단계입니다.\n",
    "#네트워크에서 댓글 부분 url 직접 가져오기\n",
    "    re=requests.get(c_url,headers=header)\n",
    "    cont=BeautifulSoup(r.content,\"html.parser\")    \n",
    "    total_comm=str(cont).split('comment\":')[1].split(\",\")[0]\n",
    "   \n",
    "    match=re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(cont))\n",
    "# 댓글을 리스트에 중첩합니다.\n",
    "    List.append(match)\n",
    "# 한번에 댓글이 20개씩 보이기 때문에 한 페이지씩 몽땅 댓글을 긁어 옵니다.\n",
    "    if int(total_comm) <= ((page) * 20):\n",
    "        result = {\"comment\" : List}\n",
    "        df = pd.DataFrame(result)\n",
    "        break\n",
    "    else : \n",
    "        page+=1\n",
    "    \n",
    "    \n",
    "    # 새로 만들 파일이름 지정\n",
    "#     outputFileName = '%s-%s-%s  %s시 %s분 %s초 merging.xlsx' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "#     df.to_excel(RESULT_PATH+outputFileName,sheet_name='sheet1')        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\"버ㄹ ㅓ지...죄없는 이를 밟고..올라간 버ㄹ ㅓ지..\", \"이런건 국가나 정부...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"남의인생 밟고올라서서 살고있는  인간쓰레기들 결국 죗값은 네들이 죽어서라도받는다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\"OOO 얼굴을 까발려야지  쓰레기경찰로\", \"그래 너거 자식들 얼마나 잘돼는지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"진범은 못잡는 무능력자들이 엄한 사람 인생 망치고 특진에 연금에 다 압수해라\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"검찰,사법부개혁도 중요하지만 경찰개혁도 중요하다...쓰레기 경찰들도 많았다..\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\"이게 나라냐?\", \"살인의 추억 영화에서 하던 ㅂ ㅓ러지 짓들이 진짜였네\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[\"인간 쓰레기들  지 자식들한테는 바르게 살라고 했겠지\", \"시간이  지났다고 양...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[\"사죄 하세요\", \"하도 기막혀 어이가 없다.\", \"공소시효라는건 과거에 컴퓨터도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[\"ㅅㅂ 말년에는 고통받는 삶을 살긴바란다\", \"당시 화성 연쇄 살인 관련 수사한 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[\"공소시효 없애라\", \"ㅋㅋㅋ  여억쉬~~   ㅁㅣㄱㅐ  종자  넘치나는  ㄱㅐ~...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[\"진짜 저 경찰때문에 한사람의 인생이 송두리째 날라가버렸는데 보상 안해주냐\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[\"공시는 지나 죄를 묻지 않아도 연금은 박탈내지 삭감 해야된다.역사단죄는 필요한것...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[\"이따위 개같은짓하라고 피같은세금 내는거아니다\", \"개만도못한ㅅㄲ들이네 . 약자를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[\"할말이없다 진짜..\", \"이런일은 인권위가 참조용하단 말야..지들이 부르짖는인권...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[\"인생 부끄럽게 살지마쇼\", \"그럼 법을 바꿔야지 .... 누구는 옥살이 20년하...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment\n",
       "0   [\"버ㄹ ㅓ지...죄없는 이를 밟고..올라간 버ㄹ ㅓ지..\", \"이런건 국가나 정부...\n",
       "1   [\"남의인생 밟고올라서서 살고있는  인간쓰레기들 결국 죗값은 네들이 죽어서라도받는다...\n",
       "2   [\"OOO 얼굴을 까발려야지  쓰레기경찰로\", \"그래 너거 자식들 얼마나 잘돼는지 ...\n",
       "3   [\"진범은 못잡는 무능력자들이 엄한 사람 인생 망치고 특진에 연금에 다 압수해라\",...\n",
       "4   [\"검찰,사법부개혁도 중요하지만 경찰개혁도 중요하다...쓰레기 경찰들도 많았다..\"...\n",
       "5   [\"이게 나라냐?\", \"살인의 추억 영화에서 하던 ㅂ ㅓ러지 짓들이 진짜였네\", \"...\n",
       "6   [\"인간 쓰레기들  지 자식들한테는 바르게 살라고 했겠지\", \"시간이  지났다고 양...\n",
       "7   [\"사죄 하세요\", \"하도 기막혀 어이가 없다.\", \"공소시효라는건 과거에 컴퓨터도...\n",
       "8   [\"ㅅㅂ 말년에는 고통받는 삶을 살긴바란다\", \"당시 화성 연쇄 살인 관련 수사한 ...\n",
       "9   [\"공소시효 없애라\", \"ㅋㅋㅋ  여억쉬~~   ㅁㅣㄱㅐ  종자  넘치나는  ㄱㅐ~...\n",
       "10  [\"진짜 저 경찰때문에 한사람의 인생이 송두리째 날라가버렸는데 보상 안해주냐\", \"...\n",
       "11  [\"공시는 지나 죄를 묻지 않아도 연금은 박탈내지 삭감 해야된다.역사단죄는 필요한것...\n",
       "12  [\"이따위 개같은짓하라고 피같은세금 내는거아니다\", \"개만도못한ㅅㄲ들이네 . 약자를...\n",
       "13  [\"할말이없다 진짜..\", \"이런일은 인권위가 참조용하단 말야..지들이 부르짖는인권...\n",
       "14  [\"인생 부끄럽게 살지마쇼\", \"그럼 법을 바꿔야지 .... 누구는 옥살이 20년하..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
